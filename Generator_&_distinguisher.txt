import torch  
import torch.nn as nn  
import pandas as pd  
import numpy as np  
from sklearn.preprocessing import LabelEncoder, StandardScaler  
from torch.utils.data import Dataset, DataLoader  

torch.manual_seed(42)
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# ---------- Function: Preprocess Data ----------
def preprocess_df(df, sample_size=100_000):
    # 1) Sample a subset of the data if it's too large
    if len(df) > sample_size:
        df = df.sample(sample_size, random_state=42).reset_index(drop=True)
    print("After sampling:", df.shape)

    # 2) Function to split IP address columns into 4 octets
    def split_ip(series, prefix):
        ip_split = series.str.split('.', expand=True)  # Split string into 4 parts
        ip_split.columns = [f"{prefix}_{i+1}" for i in range(4)]  # Rename columns
        ip_split = ip_split.astype(int)  # Convert to integers
        return ip_split

    # Apply IP splitting to source and destination IPs
    orig_h = split_ip(df['id.orig_h'], 'orig_h')
    resp_h = split_ip(df['id.resp_h'], 'resp_h')

    # Print split IPs to verify
    print("\nSplit id.orig_h:")
    print(orig_h.head())
    print("\nSplit id.resp_h:")
    print(resp_h.head())

    # 3) Label encode the target column 'label'
    label_le = LabelEncoder().fit(df['label'])

    # 4) One-hot encode categorical features
    cat_cols = ['proto', 'conn_state', 'history']
    encoders = {}  # Store label encoders
    ohe_dfs = []  # Store one-hot encoded DataFrames
    for c in cat_cols:
        le = LabelEncoder().fit(df[c])  # Fit label encoder
        encoders[c] = le
        idx = le.transform(df[c])  # Transform to integer categories
        ohe = pd.get_dummies(idx, prefix=c).astype(float)  # One-hot encode
        ohe_dfs.append(ohe)

    # 5) Normalize numeric columns
    num_cols = ['id.orig_p', 'id.resp_p', 'missed_bytes',
                'orig_pkts', 'orig_ip_bytes', 'resp_pkts', 'resp_ip_bytes']
    scaler = StandardScaler().fit(df[num_cols])  # Fit scaler on numerical data
    scaled = pd.DataFrame(scaler.transform(df[num_cols]), columns=num_cols, dtype=float)

    # 6) Concatenate all processed features into one 2D array
    features = pd.concat([orig_h, resp_h] + ohe_dfs + [scaled], axis=1)
    features = features.astype(float)
    X = torch.tensor(features.values, dtype=torch.float32)  # Convert to PyTorch tensor(2D array)
    print("Feature tensor shape:", X.shape)

    return X, scaler, encoders, num_cols, label_le, ohe_dfs

# ---------- Function: Decode synthetic data ----------
def decode_synthetic(syn_np, scaler, encoders, num_cols, label_le, ohe_dfs):
    # Clip and convert the first 8 columns back to IP octets
    orig_h = np.floor(np.clip(syn_np[:, 0:4], 0, 255)).astype(int)
    resp_h = np.floor(np.clip(syn_np[:, 4:8], 0, 255)).astype(int)

    offs = 8  # Start offset after IP octets
    decoded = {}
    # Decode one-hot encoded categorical features
    for c, ohe in zip(encoders.keys(), ohe_dfs):
        length = ohe.shape[1]
        block = syn_np[:, offs:offs + length]  # Slice the block
        idxs = np.argmax(block, axis=1)  # Get the index of highest prob
        decoded[c] = encoders[c].inverse_transform(idxs)  # Decode labels
        offs += length

    # Decode numeric features (inverse transform)
    num_block = np.clip(syn_np[:, offs:], 0, None)
    num_real = scaler.inverse_transform(num_block)

    # Reconstruct IP addresses and combine all fields into a DataFrame
    out = {
        'id.orig_h': ['.'.join(map(str, r)) for r in orig_h],
        'id.resp_h': ['.'.join(map(str, r)) for r in resp_h],
    }

    out.update(decoded)  # Add decoded categorical columns
    for i, col in enumerate(num_cols):
        out[col] = np.floor(num_real[:, i]).astype(int)  # Round and convert to int

    return pd.DataFrame(out)

# ---------- Load and clean the dataset ----------
df = pd.read_csv('C:/Users/dodo_/Downloads/CTU-IoT-Malware-Capture-1-1conn.log.labeled.csv')  # Load CSV
df = df.loc[:, ~df.columns.str.contains('^Unnamed')]  # Remove unnamed columns
df.columns = df.columns.str.strip()  # Strip whitespace from column names

# Preprocess the data
X, scaler, encoders, num_cols, label_le, ohe_dfs = preprocess_df(df)

# ---------- Create PyTorch Dataset and DataLoader ----------
class NetDataset(Dataset):
    def __init__(self, X):
        self.X = X  # Save data

    def __len__(self):
        return len(self.X)  # Number of samples

    def __getitem__(self, i):
        return self.X[i]  # Return i-th sample

dl = DataLoader(NetDataset(X), batch_size=256, shuffle=True)  # Create batched data loader

# ---------- Define Generator and Discriminator Models ----------
noise_dim = 100  # Dimension of the noise vector
feature_dim = X.shape[1]  # Number of features in input data

class Generator(nn.Module):
    def __init__(self):
        super().__init__()
        # Define a simple feedforward network
        self.net = nn.Sequential(
            nn.Linear(noise_dim, 256), nn.LeakyReLU(0.2),
            nn.Linear(256, 512), nn.LeakyReLU(0.2),
            nn.Linear(512, feature_dim)  # Output size matches feature dimension
        )

    def forward(self, z):
        return self.net(z)  # Forward pass

class Discriminator(nn.Module):
    def __init__(self):
        super().__init__()
        # Define the discriminator network
        self.net = nn.Sequential(
            nn.Linear(feature_dim, 512), nn.LeakyReLU(0.2),
            nn.Linear(512, 256), nn.LeakyReLU(0.2),
            nn.Linear(256, 1), nn.Sigmoid()  # Output probability
        )

    def forward(self, x):
        return self.net(x)  # Forward pass

# represent the generator and discriminator
gen = Generator().to(device)
disc = Discriminator().to(device)

# ---------- Setup Training ----------
opt_g = torch.optim.Adam(gen.parameters(), lr=2e-4, betas=(0.5, 0.999))  # Optimizer for generator
opt_d = torch.optim.Adam(disc.parameters(), lr=2e-4, betas=(0.5, 0.999))  # Optimizer for discriminator
crit = nn.BCELoss()  # Binary cross-entropy loss
best_g = 1e9  # Track best generator loss
#uses the Adam optimization algorithm, which is an advanced version of gradient descent (faster and adaptive).
#These are the weights of the generator model that the optimizer will update.
#lr=2e-4: Learning rate = 0.0002. It controls how big the updates are when optimizing.
#betas=(0.5, 0.999): These are Adam-specific parameters that control how momentum and past gradients are used (helps with training stability in GANs).

# ---------- Training Loop ----------
for e in range(50):  # Run for 50 epochs
    gl, dloss = [], []  # Track losses
    for real in dl:  # Loop through data batches
        real = real.to(device)
        bs = real.size(0)  # Batch size
        valid = torch.ones(bs, 1, device=device)  # Real label = 1
        fake = torch.zeros(bs, 1, device=device)  # Fake label = 0

        # Train Discriminator
        opt_d.zero_grad()
        pred_r = disc(real)  # Predict on real data
        loss_r = crit(pred_r, valid)  # Loss on real

        z = torch.randn(bs, noise_dim, device=device)  # Random noise
        pred_f = disc(gen(z).detach())  # Predict on fake (detach to freeze G)
        loss_f = crit(pred_f, fake)  # Loss on fake

        (loss_r + loss_f).backward()  # Backprop combined loss
        opt_d.step()

        # Train Generator
        opt_g.zero_grad()
        pred_g = disc(gen(torch.randn(bs, noise_dim, device=device)))  # Generate fake and predict
        loss_g = crit(pred_g, valid)  # Generator tries to fool discriminator
        loss_g.backward()
        opt_g.step()

        gl.append(loss_g.item())  # Track generator loss
        dloss.append(((loss_r + loss_f) / 2).item())  # Track average discriminator loss

    ag, ad = np.mean(gl), np.mean(dloss)  # Average losses per epoch
    print(f"Epoch {e+1:02d} | D {ad:.3f} | G {ag:.3f}")
    if ag < best_g:  # Save generator if it improves
        best_g = ag
        torch.save(gen.state_dict(), 'generator.pt')

print("Done – best G loss:", best_g)

# ---------- Generate Synthetic Data ----------
gen.load_state_dict(torch.load('generator.pt', map_location=device))  # Load best generator
gen.eval()  # Set to eval mode

need = 20  # Number of synthetic samples
with torch.no_grad():  # Inference mode
    z = torch.randn(need, noise_dim, device=device)  # Random noise
    raw = gen(z).cpu().numpy()  # Generate and move to CPU

# ---------- Decode and Save Synthetic Data ----------
syn_df = decode_synthetic(raw, scaler, encoders, num_cols, label_le, ohe_dfs)  # Convert raw output to readable form
syn_df['label'] = ['Benign']*10 + ['Malicious']*10  # Assign dummy labels

syn_df.to_csv('synthetic_traffic.csv', index=False)  # Save to CSV
print("Saved synthetic_traffic.csv:")
print(syn_df.head(10))  # Show first 10 rows

